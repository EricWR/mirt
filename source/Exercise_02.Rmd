Exercise 2
========================================================

This exercise requires setting up `mirt()` for estimating MIRT models with the polytomous data 
provided, and requires applying some useful generic functions suitable for exploratory and 
confirmatory models. The data is available in the `Exercise_02.Rdata` file. 

Load the dataset into R using the `load()` function, and inspect the defined objects.
The object named `data` represents a 20-item Likert scale questionnaire, providing a 5 point rating
scale for each item describing how unethical a particular situation is. 
For example, a question might be "Is it wrong for someone 
to steal a loaf of bread for a starving family of five?", or "Is it wrong if you don't finish your 
homework on time for class?". A coding of 1 = 'strongly disagree', 2 = 'disagree',
3 = 'neutral', 4 = 'agree', and 5 = 'strongly agree'. 

The test was designed to ask increasingly more
unethical questions, so that in the early stages participants should largely 'disagree', while near the
end they should largely 'agree'. Question names also indicate whether the situation was about the 
behaviour of others (`others`), or behaviour of the individual (`self`).

```{r setup, include=FALSE}
# set to true to show my answers
showcode <- TRUE
```

### 1) Explore the dataset, and fit a unidimenional model using the 'graded' response model (the default`itemtype` for polytomous data).

```{r Q1, include=showcode, eval=showcode}
library(mirt)
load('Exercise_02.Rdata')
psych::describe(data)
mod <- mirt(data, 1)
mod
coef(mod)
```

### 2) Test the dimensionality of the data by extracting 2 and 3 exploratory factors. Use `anova()` to perform a likelihood ratio tests and to compare information statistics, such as AIC and BIC. Note that for the 3-factor solution it may be beneficial to set `TOL = .001` to drop the EM convergence criteria.

```{r Q2, include=showcode, eval=showcode}
mod2 <- mirt(data, 2)
mod2
# much better fit with 2 dimensions, with all criteria
anova(mod, mod2)

# solution has a fairly flat surface at the ML...a good sign of a mis-specified model
mod3 <- mirt(data, 3, TOL = .001, verbose = FALSE)
# not significantly diff from 2 dimensional model, and all information stats suggest 2 dims
anova(mod2, mod3)
```

### 3) Using the 2-factor model, attempt to rotate the solution to an interpretable orientation using the `summary()` function with a `rotate = ` argument. Try using the `varimax` and `oblimin` criteria, and suppress coefficients with the `suppress = ` argument (takes a number and replaces values below with NA's). Do you notice any discernable pattern in the loadings?

```{r Q3, include=showcode, eval=showcode}
summary(mod2, rotate = 'varimax')
summary(mod2, rotate = 'oblimin')
summary(mod2, rotate = 'oblimin', suppress = .2)
```

### 4) Given the pattern you observed in question 3, create a more restricted confirmatory model using the `mirt.model()` syntax. Use `anova()` to determine if the confirmatory model fits significantly worse than the exploratory model, and inspect and interpret the estimated coefficients.

```{r Q4, include=showcode, tidy=FALSE, eval=showcode}
model <- mirt.model('self = 1,3,4,7,9,10,15,16,18,19,20
                     others = 2,3,5,6,8,11-14,17
                     COV = self*others')
cmod <- mirt(data, model)
anova(mod2, cmod)
coef(cmod)
#coefs as a data.frame
coef(cmod, as.data.frame=TRUE)
```

### 5) Compute EAP factor scores for the confirmatory model, returning both the tabulated form (a potentially  smaller data.frame consisting of estimates for each unique response pattern) and the complete data form (applied to the original dataset, returning estimates for each person).

```{r Q5, include=showcode, eval=showcode}
tab <- fscores(cmod, method = 'EAP')
head(tab)

full <- fscores(cmod, method = 'EAP', full.scores = TRUE)
head(full)
```

### 6) You would like to know if the item categories can be simplified by forcing the spacing between each category in each item to be the same, and using only 1 intercept to represent the total rating 'easiness' (these models are generally called rating scale models, and are useful for creating parsimonious models with more elegant interpretations). Fit a graded rating scale model (`itemtype = 'grsm'`) to the data, test whether the model fit significantly worse than the confirmatory model above, and inspect/interpret the coefficients. 

```{r Q6, include=showcode, eval=showcode}
rating <- mirt(data, model, itemtype = 'grsm')

# save 57 degrees of freedom! Maybe more if slopes could also be constrained to be equal...
anova(rating, cmod)

# c parameter indicates the relative easiness of the item 
# (i.e., tendencies to respond in higher categories). Shows that the items systematically 
# move from subjects selecting strongly disagree to strongly agree
coef(rating)

itemplot(rating, 1)
itemplot(rating, 10)
itemplot(rating, 20)

# dropping slopes exactly equal to 0 (easier to visualize)
itemplot(rating, 1, drop.zeros = TRUE)
itemplot(rating, 10, drop.zeros = TRUE)
itemplot(rating, 20, drop.zeros = TRUE)
```
