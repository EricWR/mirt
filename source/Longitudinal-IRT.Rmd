IRT approaches to longitudinal data
===================================

Two approaches are demonstrated here for longitudinal data. A direct modeling approach, 
where a latent variable is estimated for each time point, and a linear latent growth curve 
approach which estimates the variability of person trajectories. The latter is limited to
only 3 dimensions for any number of time points, whereas the former increases in complexity
as the time points increase.

## Direct estimation of latent factors for each wave

Two tier approach used in Cai 2010's two-tier article. This example requires 4 dimensions 
of integration (1 for each time point + residual item factor)

```{r}
library(mirt)
load('longitudinal-IRT.Rdata')
head(dat)
```

```{r}
model <- 'Time1 = 1-20
          Time2 = 21-40
          Time3 = 41-60
          COV = Time1*Time2*Time3, Time2*Time2, Time3*Time3
          MEAN = Time2, Time3'
itemloadings <- rep(1:20, times = 3) 

#construct constraints dynatically
#obtain starting values
sv <- bfactor(dat, itemloadings, model, pars='values')

# set up within time constraints
wtconstr <- sv$parnum[(sv$name == 'a1' | sv$name == 'a2' | sv$name == 'a3') & sv$est]

# create constraint list
constraints <- list()
itemnames <- colnames(dat)
pick <- c(0,20,40)
for(i in 1:20){
    
    # accross time item constraints
    constraints[[paste0('slope.', i)]] <- sv$parnum[sv$name == paste0('a',3+i) & sv$est]
    for(j in 1:2){
        constraints[[paste0('intercept.', i, '_', j)]] <- 
            sv$parnum[sv$name == paste0('d',j) & (sv$item %in% itemnames[pick + i]) & sv$est]
    }
    
    #across time constraints
    constraints[[paste0('Time.', i)]] <- wtconstr[pick + i]
}

#estimate model (low quadrature just as an example; not as accurate.
#                Could use as starting values for model with higher quadpts via pars=mod2values(mod))
(mod <- bfactor(dat, itemloadings, model, constrain=constraints, quadpts=7, TOL=1e-3,
                optimizer = 'nlminb'))

coef(mod, simplify=TRUE)

coef(mod)$GroupPars[,c('MEAN_2', 'MEAN_3', 'COV_11', 'COV_21', 'COV_31', 'COV_22', 
                       'COV_32', 'COV_33')]
```

When computing latent trait scores it's best to use a method that isn't as affected by higher dimensionality, such as the MAP approach. 

## Linear latent growth curve IRT 

Specific factors represent residula terms per item. Requires only 3 dimensions 
(max 3 dimensions of integration for unidimensional models).

```{r}
model <- mirt.model('Intercept = 1-60
                     Slope = 1-60
                     COV = Intercept*Slope, Intercept*Intercept, Slope*Slope
                     MEAN = Intercept, Slope')
itemloadings <- rep(1:20, times = 3) 

#construct constraints dynatically
#obtain starting values
sv <- bfactor(dat, itemloadings, model, pars='values')

# time constants for linear trend
sv$value[sv$name == 'a1'] <- 1
sv$est[sv$name == 'a1'] <- FALSE
sv$value[sv$name == 'a2'] <- rep(0:2, each=20)
sv$est[sv$name == 'a2'] <- FALSE

# create constraint list
constraints <- list()
itemnames <- colnames(dat)
pick <- c(0,20,40)
for(i in 1:20){    
    # accross time item constraints
    constraints[[paste0('slope.', i)]] <- sv$parnum[sv$name == paste0('a',2+i) & sv$est]
    for(j in 1:2){
        constraints[[paste0('intercept.', i, '_', j)]] <- 
            sv$parnum[sv$name == paste0('d',j) & (sv$item %in% itemnames[pick + i]) & sv$est]
    }
}

#estimate model (low quadrature just as an example; not as accurate.
#                Could use as starting values for model with higher quadpts via pars=mod2values(mod))
mod2 <- bfactor(dat, itemloadings, model, constrain=constraints, quadpts=9, pars=sv, 
                optimizer = 'nlminb')

coef(mod2, simplify=TRUE)

coef(mod2)$GroupPars[,c('MEAN_1', 'MEAN_2', 'COV_11', 'COV_21', 'COV_22')]
```
